{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(text_chunk, split_index=1):\n",
    "    \"\"\"Splits text into two chunks representing the input to be fed \n",
    "    into the NN, and it's target label.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> split_input_target(\"Python\")\n",
    "    \"Pytho\", \"ython\"\n",
    "    \"\"\"\n",
    "    input_text = text_chunk[:-split_index]\n",
    "    target_text = text_chunk[split_index:]\n",
    "    \n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Create text using a character-based recurrent neural network. We will use the novel Great Expectations by Charles Dickens. We will train the network on this text so that, if we give it a character sequence such as thousan, it will produce the next character in the sequence, d. This process can be continued, and longer sequences of text created by calling the model repeatedly on the evolving sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/1400/1400-0.txt\n",
      "1056768/1049619 [==============================] - 2s 2us/step\n"
     ]
    }
   ],
   "source": [
    "text_url = 'https://www.gutenberg.org/files/1400/1400-0.txt' # Great Expectations by Charles Dickens\n",
    "file_path = tf.keras.utils.get_file('1400-0.txt', text_url) # Downloads to cache if it isn't already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text: 1013445 characters\n"
     ]
    }
   ],
   "source": [
    "with open(file_path) as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "print(f'Lenght of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 824 characters are not part of the book. They are notes and licencing information from Project Gutenberg and shouldn't be part of training so lets remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[824:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "\n",
      "My father's family name being Pirrip, and my Christian name Philip, my\n",
      "infant tongue could make of both names nothing longer or more explicit\n",
      "than Pip. So, I called myself Pip, and came to be called Pip.\n",
      "\n",
      "I give Pirrip as my father's family name, on the authority of his\n",
      "tombstone and my s\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets create a mapping from char to int so the characters can represented as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(text)) # Gets distinct values\n",
    "char_to_int = {char:i for i, char in enumerate(unique_chars)}\n",
    "int_to_char = {v:k for k, v in char_to_int.items()}\n",
    "index_to_char = np.array(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n': 0\n",
      "' ' : 1\n",
      "'!' : 2\n",
      "'$' : 3\n",
      "'%' : 4\n",
      "'&' : 5\n",
      "\"'\" : 6\n",
      "'(' : 7\n",
      "')' : 8\n",
      "'*' : 9\n"
     ]
    }
   ],
   "source": [
    "# Sample output\n",
    "for (k, v), _ in zip(char_to_int.items(), range(10)):\n",
    "    print(f\"{repr(k):4s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My father's fami ----> [ 0 40 78  1 59 54 73 61 58 71  6 72  1 59 54 66 62]\n"
     ]
    }
   ],
   "source": [
    "book_vector = np.array([char_to_int[char] for char in text])\n",
    "\n",
    "# Sample mapping\n",
    "print(f\"{text[10:27]} ----> {book_vector[10:27]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text) // sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "h\n",
      "a\n",
      "p\n",
      "t\n",
      "e\n",
      "r\n",
      " \n"
     ]
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_vector)\n",
    "\n",
    "# Sanity check\n",
    "for char in char_dataset.take(8):\n",
    "    print(int_to_char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we're adding 1 to the sequence in this function, the batch size is 101\n",
    "sequences = char_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"Chapter I\\n\\nMy father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue coul\"\n",
      "Target data: \"hapter I\\n\\nMy father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue could\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()]))) #101 characters\n",
    "    print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 input: 30 ('C') expected output: 61 ('h')\n",
      "Step    1 input: 61 ('h') expected output: 54 ('a')\n",
      "Step    2 input: 54 ('a') expected output: 69 ('p')\n",
      "Step    3 input: 69 ('p') expected output: 73 ('t')\n",
      "Step    4 input: 73 ('t') expected output: 58 ('e')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_index, target_index) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(f\"Step {i:4d}\", end=\"\")\n",
    "    print(f\" input: {input_index} ({repr(int_to_char[input_index.numpy()])})\", end=\"\")\n",
    "    print(f\" expected output: {target_index} ({repr(int_to_char[target_index.numpy()])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training set up\n",
    "\n",
    "# How many characters in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# The number of training steps taken in each epoch\n",
    "steps_per_epoch = examples_per_epoch // batch\n",
    "\n",
    "# TF data maintains a buffer in memory to shuffle data since it's designed\n",
    "# to work with the possibility of endless data\n",
    "buffer = 1000\n",
    "\n",
    "dataset = dataset.shuffle(buffer).batch(batch, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vocabulary length in characters\n",
    "vocabulary_length = len(unique_chars)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dimension = 256\n",
    "\n",
    "# The number of recurrent neural network units\n",
    "num_rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not found, falling back to CPU\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    recurrent_nn = tf.compat.v1.keras.layers.CuDNNGRU\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    import functools\n",
    "    recurrent_nn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    print(\"GPU not found, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "![DNN Layout](images/dnn-layout.png \"DNN Layout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, num_rnn_units, batch_size, recurrent_nn):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "                                 recurrent_nn(num_rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "                                 tf.keras.layers.Dense(vocab_size)\n",
    "                                ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n",
    "                    num_rnn_units=num_rnn_units, batch_size=batch_size, recurrent_nn=recurrent_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 84) # (batch, sequence_length, vocabulary_length)\n"
     ]
    }
   ],
   "source": [
    "for batch_input_example, batch_target_example in dataset.take(1):\n",
    "    batch_predictions_example = model(batch_input_example)\n",
    "    print(batch_predictions_example.shape, \"# (batch, sequence_length, vocabulary_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "unified_gru (UnifiedGRU)     (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 4,045,908\n",
      "Trainable params: 4,045,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84 unique chars * 256 embedding dimms = 21,504\n",
    "\n",
    "1024 GRU units * 84 unique chars + 84 bias units = 86,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 84)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:  10.125925\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(batch_target_example, batch_predictions_example)\n",
    "print(\"Prediction shape: \", batch_predictions_example.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss: \", batch_loss_example.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 291s 2s/step - loss: 2.7153\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 366s 2s/step - loss: 1.9887\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 329s 2s/step - loss: 1.7063\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 314s 2s/step - loss: 1.5364\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 266s 2s/step - loss: 1.4302\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 258s 2s/step - loss: 1.3569\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 269s 2s/step - loss: 1.3000\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 263s 2s/step - loss: 1.2509\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 284s 2s/step - loss: 1.2052\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 310s 2s/step - loss: 1.1603\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "\n",
    "To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n",
    "                    num_rnn_units=num_rnn_units, batch_size=batch_size, recurrent_nn=recurrent_nn)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
