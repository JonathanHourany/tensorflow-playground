{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Create text using a character-based recurrent neural network. We will use the novel Great Expectations by Charles Dickens. We will train the network on this text so that, if we give it a character sequence such as thousan, it will produce the next character in the sequence, d. This process can be continued, and longer sequences of text created by calling the model repeatedly on the evolving sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_url = 'https://www.gutenberg.org/files/1400/1400-0.txt' # Great Expectations by Charles Dickens\n",
    "file_path = tf.keras.utils.get_file('1400-0.txt', text_url) # Downloads to cache if it isn't already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text: 1013445 characters\n"
     ]
    }
   ],
   "source": [
    "with open(file_path) as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "print(f'Lenght of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 824 characters are not part of the book. They are notes and licencing information from Project Gutenberg and shouldn't be part of training so lets remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[824:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "\n",
      "My father's family name being Pirrip, and my Christian name Philip, my\n",
      "infant tongue could make of both names nothing longer or more explicit\n",
      "than Pip. So, I called myself Pip, and came to be called Pip.\n",
      "\n",
      "I give Pirrip as my father's family name, on the authority of his\n",
      "tombstone and my s\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets create a mapping from char to int so the characters can represented as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(text)) # Gets distinct values\n",
    "char_to_int = {char:i for i, char in enumerate(unique_chars)}\n",
    "int_to_char = {v:k for k, v in char_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n': 0\n",
      "' ' : 1\n",
      "'!' : 2\n",
      "'$' : 3\n",
      "'%' : 4\n",
      "'&' : 5\n",
      "\"'\" : 6\n",
      "'(' : 7\n",
      "')' : 8\n",
      "'*' : 9\n"
     ]
    }
   ],
   "source": [
    "# Sample output\n",
    "for (k, v), _ in zip(chars_to_int.items(), range(10)):\n",
    "    print(f\"{repr(k):4s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My father's fami ----> [ 0 40 78  1 59 54 73 61 58 71  6 72  1 59 54 66 62]\n"
     ]
    }
   ],
   "source": [
    "book_vector = np.array([chars_to_int[char] for char in text])\n",
    "\n",
    "# Sample mapping\n",
    "print(f\"{text[10:27]} ----> {book_vector[10:27]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text) // sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "h\n",
      "a\n",
      "p\n",
      "t\n",
      "e\n",
      "r\n",
      " \n"
     ]
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_vector)\n",
    "\n",
    "# Sanity check\n",
    "for char in char_dataset.take(8):\n",
    "    print(int_to_char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we're adding 1 to the sequence in this function, the batch size is 101\n",
    "sequences = char_dataset.batch(sequence_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"The function returns the text that we have been working with, \n",
    "    together with the same text, but shifted one character along\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
