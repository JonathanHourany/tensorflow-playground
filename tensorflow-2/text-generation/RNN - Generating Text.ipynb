{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(text_chunk, split_index=1):\n",
    "    \"\"\"Splits text into two chunks representing the input to be fed \n",
    "    into the NN, and it's target label.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> split_input_target(\"Python\")\n",
    "    \"Pytho\", \"ython\"\n",
    "    \"\"\"\n",
    "    input_text = text_chunk[:-split_index]\n",
    "    target_text = text_chunk[split_index:]\n",
    "    \n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "Create text using a character-based recurrent neural network. We will use the novel Great Expectations by Charles Dickens. We will train the network on this text so that, if we give it a character sequence such as thousan, it will produce the next character in the sequence, d. This process can be continued, and longer sequences of text created by calling the model repeatedly on the evolving sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_url = 'https://www.gutenberg.org/files/1400/1400-0.txt' # Great Expectations by Charles Dickens\n",
    "file_path = tf.keras.utils.get_file('1400-0.txt', text_url) # Downloads to cache if it isn't already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text: 1013445 characters\n"
     ]
    }
   ],
   "source": [
    "with open(file_path) as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "print(f'Lenght of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 824 characters are not part of the book. They are notes and licencing information from Project Gutenberg and shouldn't be part of training so lets remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[824:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "\n",
      "My father's family name being Pirrip, and my Christian name Philip, my\n",
      "infant tongue could make of both names nothing longer or more explicit\n",
      "than Pip. So, I called myself Pip, and came to be called Pip.\n",
      "\n",
      "I give Pirrip as my father's family name, on the authority of his\n",
      "tombstone and my s\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets create a mapping from char to int so the characters can represented as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(text)) # Gets distinct values\n",
    "char_to_int = {char:i for i, char in enumerate(unique_chars)}\n",
    "int_to_char = {v:k for k, v in char_to_int.items()}\n",
    "index_to_char = np.array(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n': 0\n",
      "' ' : 1\n",
      "'!' : 2\n",
      "'$' : 3\n",
      "'%' : 4\n",
      "'&' : 5\n",
      "\"'\" : 6\n",
      "'(' : 7\n",
      "')' : 8\n",
      "'*' : 9\n"
     ]
    }
   ],
   "source": [
    "# Sample output\n",
    "for (k, v), _ in zip(char_to_int.items(), range(10)):\n",
    "    print(f\"{repr(k):4s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My father's fami ----> [ 0 40 78  1 59 54 73 61 58 71  6 72  1 59 54 66 62]\n"
     ]
    }
   ],
   "source": [
    "book_vector = np.array([char_to_int[char] for char in text])\n",
    "\n",
    "# Sample mapping\n",
    "print(f\"{text[10:27]} ----> {book_vector[10:27]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "sequence_length = 100\n",
    "examples_per_epoch = len(text) // sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "h\n",
      "a\n",
      "p\n",
      "t\n",
      "e\n",
      "r\n",
      " \n"
     ]
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(book_vector)\n",
    "\n",
    "# Sanity check\n",
    "for char in char_dataset.take(8):\n",
    "    print(int_to_char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we're adding 1 to the sequence in this function, the batch size is 101\n",
    "sequences = char_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"Chapter I\\n\\nMy father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue coul\"\n",
      "Target data: \"hapter I\\n\\nMy father's family name being Pirrip, and my Christian name Philip, my\\ninfant tongue could\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()]))) #101 characters\n",
    "    print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0 input: 30 ('C') expected output: 61 ('h')\n",
      "Step    1 input: 61 ('h') expected output: 54 ('a')\n",
      "Step    2 input: 54 ('a') expected output: 69 ('p')\n",
      "Step    3 input: 69 ('p') expected output: 73 ('t')\n",
      "Step    4 input: 73 ('t') expected output: 58 ('e')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_index, target_index) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(f\"Step {i:4d}\", end=\"\")\n",
    "    print(f\" input: {input_index} ({repr(int_to_char[input_index.numpy()])})\", end=\"\")\n",
    "    print(f\" expected output: {target_index} ({repr(int_to_char[target_index.numpy()])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training set up\n",
    "\n",
    "# How many characters in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# The number of training steps taken in each epoch\n",
    "steps_per_epoch = examples_per_epoch // batch_size\n",
    "\n",
    "# TF data maintains a buffer in memory to shuffle data since it's designed\n",
    "# to work with the possibility of endless data\n",
    "buffer = 1000\n",
    "\n",
    "dataset = dataset.shuffle(buffer).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vocabulary length in characters\n",
    "vocabulary_length = len(unique_chars)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dimension = 256\n",
    "\n",
    "# The number of recurrent neural network units\n",
    "num_rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    recurrent_nn = tf.compat.v1.keras.layers.CuDNNGRU\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    import functools\n",
    "    recurrent_nn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    print(\"GPU not found, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "![DNN Layout](images/dnn-layout.png \"DNN Layout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, num_rnn_units, batch_size, recurrent_nn):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "                                 recurrent_nn(num_rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "                                 tf.keras.layers.Dense(vocab_size)\n",
    "                                ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n",
    "                    num_rnn_units=num_rnn_units, batch_size=batch_size, recurrent_nn=recurrent_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 84) # (batch, sequence_length, vocabulary_length)\n"
     ]
    }
   ],
   "source": [
    "for batch_input_example, batch_target_example in dataset.take(1):\n",
    "    batch_predictions_example = model(batch_input_example)\n",
    "    print(batch_predictions_example.shape, \"# (batch, sequence_length, vocabulary_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 4,045,908\n",
      "Trainable params: 4,045,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84 unique chars * 256 embedding dimms = 21,504\n",
    "\n",
    "1024 GRU units * 84 unique chars + 84 bias units = 86,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 84)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:  4.429464\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(batch_target_example, batch_predictions_example)\n",
    "print(\"Prediction shape: \", batch_predictions_example.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss: \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.1104\n",
      "Epoch 2/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0667\n",
      "Epoch 3/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 1.0313\n",
      "Epoch 4/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 1.0052\n",
      "Epoch 5/30\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9743\n",
      "Epoch 6/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9432\n",
      "Epoch 7/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.9190\n",
      "Epoch 8/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8980\n",
      "Epoch 9/30\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8776\n",
      "Epoch 10/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8587\n",
      "Epoch 11/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8420\n",
      "Epoch 12/30\n",
      "156/156 [==============================] - 9s 60ms/step - loss: 0.8309\n",
      "Epoch 13/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8190\n",
      "Epoch 14/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.8034\n",
      "Epoch 15/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7859\n",
      "Epoch 16/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7703\n",
      "Epoch 17/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7632\n",
      "Epoch 18/30\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7574\n",
      "Epoch 19/30\n",
      "156/156 [==============================] - 9s 58ms/step - loss: 0.7499\n",
      "Epoch 20/30\n",
      "156/156 [==============================] - 9s 59ms/step - loss: 0.7462\n",
      "Epoch 21/30\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7412\n",
      "Epoch 22/30\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 0.7398\n",
      "Epoch 23/30\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 0.7391\n",
      "Epoch 24/30\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7395\n",
      "Epoch 25/30\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7385\n",
      "Epoch 26/30\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7367\n",
      "Epoch 27/30\n",
      "156/156 [==============================] - 9s 57ms/step - loss: 0.7353\n",
      "Epoch 28/30\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 0.7331\n",
      "Epoch 29/30\n",
      "156/156 [==============================] - 9s 55ms/step - loss: 0.7317\n",
      "Epoch 30/30\n",
      "156/156 [==============================] - 9s 56ms/step - loss: 0.7322\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "\n",
    "To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = build_model(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n",
    "                    num_rnn_units=num_rnn_units, batch_size=1, recurrent_nn=recurrent_nn)\n",
    "pred_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "pred_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char_to_int[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(int_to_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter I\\n\\nMy fearful lady,”\\n Trouble demanning to be a\\ngentman else may led me in, as no no person what I had never yet for\\nmy porter with the upon whom he love Joseth. But what do you\\nmake out that has been hair, and she gaved a little brolder; the remarkable signs were assee's. But ships,\\nHaving was, the way by the streature word, working about him with by\\nthe chair and wisdom over Mrs. M. “Two pace tilted me before\\nsuch. Estella's walking in the most behile\\ntime as ever became so much depends upon him comply with the rest, how take a\\nbreakfast.\\n\\nJah, was thisked by the Alphantoof.\\nNe at last I reversed myself as to\\nhold one to eat, wher a sense I felt that Herbe, all was not at all\\nKickness, to off and put it down to him, and I kept my eyes as a rehonerate\\nto look in a low cloak. Theye easted dread of it,\\nwhich wonder why I had felt for him in then chain upon her over\\nand withered alit ideas, copying\\nor face than I don't say so, Pip!”\\n\\n“Do you know where to do any given--” said she, “and \""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(pred_model, 'Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two LSTM stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    lstm_nn = tf.compat.v1.keras.layers.CuDNNLSTM\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    import functools\n",
    "    lstm_nn = functools.partial(tf.keras.CuDNNLSTM.GRU, recurrent_activation='sigmoid')\n",
    "    print(\"GPU not found, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, num_rnn_units, batch_size, recurrent_nn):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "                                 recurrent_nn(num_rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "                                 recurrent_nn(num_rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "                                 tf.keras.layers.Dense(vocab_size)\n",
    "                                ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model_lstm(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n",
    "                    num_rnn_units=num_rnn_units, batch_size=batch_size, recurrent_nn=lstm_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 84)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:  4.429464\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(batch_target_example, batch_predictions_example)\n",
    "print(\"Prediction shape: \", batch_predictions_example.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss: \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './lstm_training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (64, None, 1024)          5251072   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (64, None, 1024)          8396800   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 13,755,476\n",
      "Trainable params: 13,755,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "156/156 [==============================] - 22s 141ms/step - loss: 2.7787\n",
      "Epoch 2/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 1.9429\n",
      "Epoch 3/30\n",
      "156/156 [==============================] - 21s 136ms/step - loss: 1.5887\n",
      "Epoch 4/30\n",
      "156/156 [==============================] - 21s 136ms/step - loss: 1.4260\n",
      "Epoch 5/30\n",
      "156/156 [==============================] - 21s 138ms/step - loss: 1.3301\n",
      "Epoch 6/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 1.2581\n",
      "Epoch 7/30\n",
      "156/156 [==============================] - 21s 136ms/step - loss: 1.1958\n",
      "Epoch 8/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 1.1371\n",
      "Epoch 9/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 1.0845\n",
      "Epoch 10/30\n",
      "156/156 [==============================] - 21s 136ms/step - loss: 1.0379\n",
      "Epoch 11/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 0.9899\n",
      "Epoch 12/30\n",
      "156/156 [==============================] - 22s 140ms/step - loss: 0.9329\n",
      "Epoch 13/30\n",
      "156/156 [==============================] - 22s 138ms/step - loss: 0.8771\n",
      "Epoch 14/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 0.8239\n",
      "Epoch 15/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 0.7761\n",
      "Epoch 16/30\n",
      "156/156 [==============================] - 21s 137ms/step - loss: 0.7279\n",
      "Epoch 17/30\n",
      "111/156 [====================>.........] - ETA: 6s - loss: 0.6826"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (1024, 4096) and (1024, 84) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-311857121bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m pred_model = build_model_lstm(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n\u001b[1;32m      2\u001b[0m                     num_rnn_units=num_rnn_units, batch_size=1, recurrent_nn=lstm_nn)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m         raise NotImplementedError(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         graph_view=self._graph_view)\n\u001b[1;32m   1095\u001b[0m     base.CheckpointPosition(checkpoint=checkpoint, proto_id=0).restore(\n\u001b[0;32m-> 1096\u001b[0;31m         self._graph_view.root)\n\u001b[0m\u001b[1;32m   1097\u001b[0m     load_status = CheckpointLoadStatus(\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    777\u001b[0m           ._single_restoration_from_checkpoint_position(\n\u001b[1;32m    778\u001b[0m               \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m               visit_queue=visit_queue)))\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_single_restoration_from_checkpoint_position\u001b[0;34m(self, checkpoint_position, visit_queue)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;31m# restoration on to our dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_uid\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_uid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_position\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_uid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore_ops\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m      python_saveables) = self._gather_ops_or_named_saveables()\n\u001b[1;32m    392\u001b[0m     restore_ops.extend(self._checkpoint.restore_saveables(\n\u001b[0;32m--> 393\u001b[0;31m         tensor_saveables, python_saveables))\n\u001b[0m\u001b[1;32m    394\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[1;32m    160\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[1;32m    161\u001b[0m       new_restore_ops = functional_saver.restore_from_saveable_objects(\n\u001b[0;32m--> 162\u001b[0;31m           self.save_path_tensor, validated_saveables)\n\u001b[0m\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore_from_saveable_objects\u001b[0;34m(file_prefix, saveable_objects)\u001b[0m\n\u001b[1;32m    135\u001b[0m                                         structured_restored_tensors):\n\u001b[1;32m    136\u001b[0m     restore_ops.append(saveable.restore(restored_tensors,\n\u001b[0;32m--> 137\u001b[0;31m                                         restored_shapes=None))\n\u001b[0m\u001b[1;32m    138\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mrestored_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[0;32m--> 115\u001b[0;31m           self.handle_op, self._var_shape, restored_tensor)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mshape_safe_assign_variable_handle\u001b[0;34m(handle, shape, value, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m   return gen_resource_variable_ops.assign_variable_op(handle,\n\u001b[1;32m    274\u001b[0m                                                       \u001b[0mvalue_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \"\"\"\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (1024, 4096) and (1024, 84) are incompatible"
     ]
    }
   ],
   "source": [
    "pred_model = build_model_lstm(vocab_size=len(unique_chars), embedding_dim=embedding_dimension, \n",
    "                    num_rnn_units=num_rnn_units, batch_size=1, recurrent_nn=lstm_nn)\n",
    "pred_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "pred_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(pred_model, 'Chapter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
